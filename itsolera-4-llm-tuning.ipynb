{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# prompt: install bitsandbytes library\n\n!pip install bitsandbytes\n!pip install peft\n!pip install evaluate\n# !pip install pytorch\n!pip install requests","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Log in using your Hugging Face token\nlogin(token='PUT YOUR HUGGING FACE TOKEN HERE')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:36:34.286756Z","iopub.execute_input":"2024-10-27T01:36:34.287599Z","iopub.status.idle":"2024-10-27T01:36:34.480111Z","shell.execute_reply.started":"2024-10-27T01:36:34.287556Z","shell.execute_reply":"2024-10-27T01:36:34.479209Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"id2label = {\n    0: \"SADNESS\",\n    1: \"JOY\",\n    2: \"LOVE\",\n    3: \"ANGER\",\n    4: \"FEAR\",\n    5: \"SURPRISE\"\n}\n\nlabel2id = {\n    \"SADNESS\": 0,\n    \"JOY\": 1,\n    \"LOVE\": 2,\n    \"ANGER\": 3,\n    \"FEAR\": 4,\n    \"SURPRISE\": 5\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:36:35.883325Z","iopub.execute_input":"2024-10-27T01:36:35.883957Z","iopub.status.idle":"2024-10-27T01:36:35.888634Z","shell.execute_reply.started":"2024-10-27T01:36:35.883920Z","shell.execute_reply":"2024-10-27T01:36:35.887706Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"ds = load_dataset(\"dair-ai/emotion\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport functools\nimport csv\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom skmultilearn.model_selection import iterative_train_test_split\nfrom datasets import load_dataset\nfrom peft import (\n    LoraConfig,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n    PeftModel\n)\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n)\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:36:39.268232Z","iopub.execute_input":"2024-10-27T01:36:39.269075Z","iopub.status.idle":"2024-10-27T01:36:39.288936Z","shell.execute_reply.started":"2024-10-27T01:36:39.269034Z","shell.execute_reply":"2024-10-27T01:36:39.288232Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# define custom batch preprocessor\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# define which metrics to compute for evaluation\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=1)\n    accuracy = accuracy_score(predictions, labels)\n    \n    f1_micro = f1_score(labels, predictions > 0, average = 'micro')\n    f1_macro = f1_score(labels, predictions > 0, average = 'macro')\n    f1_weighted = f1_score(labels, predictions > 0, average = 'weighted')\n    return {\n        'accuracy': accuracy,\n        'f1_micro': f1_micro,\n        'f1_macro': f1_macro,\n        'f1_weighted': f1_weighted\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model name\nmodel_name = 'mistralai/Mistral-7B-v0.1'\n\n# preprocess dataset with tokenizer\ndef tokenize_examples(examples, tokenizer):\n    tokenized_inputs = tokenizer(examples['text'])\n    tokenized_inputs['label'] = examples['label']\n    return tokenized_inputs\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenized_ds = ds.map(functools.partial(tokenize_examples, tokenizer=tokenizer), batched=True)\ntokenized_ds = tokenized_ds.with_format('torch')\n\n# qunatization config\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, # enable 4-bit quantization\n    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n)\n\n# lora config\nlora_config = LoraConfig(\n    r = 16, # the dimension of the low-rank matrices\n    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, # dropout probability of the LoRA layers\n    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n    task_type = 'SEQ_CLS'\n)\n\n# load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    num_labels=6,\n    device_map = \"auto\"\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.config.pad_token_id = tokenizer.pad_token_id\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define training args\ntraining_args = TrainingArguments(\n    output_dir = 'ITSOLERA-4-MISTRAL-V1-TUNING',\n    learning_rate = 1e-4,\n    per_device_train_batch_size = 64,\n    per_device_eval_batch_size = 64,\n    num_train_epochs = 4,\n    weight_decay = 0.01,\n    evaluation_strategy = 'epoch',\n    save_strategy = 'epoch',\n    load_best_model_at_end = True,\n    push_to_hub=True,\n    dataloader_num_workers=4,\n)\n\n# train\ntrainer = Trainer(\n    model = model,\n    args = training_args,\n    train_dataset = tokenized_ds['train'],\n    eval_dataset = tokenized_ds['test'],\n    tokenizer = tokenizer,\n    data_collator = data_collator,\n    compute_metrics = compute_metrics,\n    #label_weights = torch.tensor(label_weights, device=model.device)\n)\n\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **INFERENCE**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\nimport torch\n\n# Load the trained model\nmodel_path = \"Muhammad10101/ITSOLERA-4-MISTRAL-V1-TUNING\"\nmodel_path_base = \"mistralai/Mistral-7B-v0.1\"\n\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)\ndevice_map = \"auto\"\n\n# Load the model with 8-bit precision and the appropriate configuration\ntest_model = AutoModelForSequenceClassification.from_pretrained(\n    model_path_base, \n    num_labels=6, \n    device_map=device_map,\n    id2label = id2label,\n    label2id = label2id,\n    quantization_config=bnb_config  # Enables 4-bit quantization\n)\n\nmodel = PeftModel.from_pretrained(test_model, model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path_base)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:36:47.314952Z","iopub.execute_input":"2024-10-27T01:36:47.315406Z","iopub.status.idle":"2024-10-27T01:37:03.266485Z","shell.execute_reply.started":"2024-10-27T01:36:47.315342Z","shell.execute_reply":"2024-10-27T01:37:03.265636Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ebc5cbbec1f42899f741ac5ef77b412"}},"metadata":{}},{"name":"stderr","text":"Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"text = \"I am broken today\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# tokenizer = AutoTokenizer.from_pretrained(model_path_base)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:37:24.466312Z","iopub.execute_input":"2024-10-27T01:37:24.467337Z","iopub.status.idle":"2024-10-27T01:37:24.472494Z","shell.execute_reply.started":"2024-10-27T01:37:24.467293Z","shell.execute_reply":"2024-10-27T01:37:24.471346Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:37:24.733204Z","iopub.execute_input":"2024-10-27T01:37:24.733520Z","iopub.status.idle":"2024-10-27T01:37:25.044269Z","shell.execute_reply.started":"2024-10-27T01:37:24.733488Z","shell.execute_reply":"2024-10-27T01:37:25.043403Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"predicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:37:25.503852Z","iopub.execute_input":"2024-10-27T01:37:25.504371Z","iopub.status.idle":"2024-10-27T01:37:25.510625Z","shell.execute_reply.started":"2024-10-27T01:37:25.504313Z","shell.execute_reply":"2024-10-27T01:37:25.509631Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'SADNESS'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}